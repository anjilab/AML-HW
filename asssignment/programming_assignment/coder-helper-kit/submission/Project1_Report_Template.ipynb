{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for Homework 1: Classifier Agent\n",
    "\n",
    "### Name: Anjila Budathoki\n",
    "### Panther ID: 002778574\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaration of Sources and Collaboration:\n",
    "\n",
    "You should declare who you worked with in this homework. Don't submit identical works, rather study together and apply it individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Gradient Calculations\n",
    "\n",
    "The loss function to use is the cross-entropy loss, averaged over data points.\n",
    "\n",
    "You should include your work for deriving the full gradient and stochastic gradient here.\n",
    "\n",
    "You can type equations using LATEX in markdown with, e.g. $\n",
    "e = mc^2, v = \\frac{dx}{dt}\n",
    "$\n",
    "\n",
    "$l(w, (x,y)) = - (ylog   \\hat{p}_w(x)  +  log(1 - \\hat{p}_w(x) )(1-y))$\n",
    "\n",
    "$\\dfrac{ \\partial \\hat{y} }{\\partial w} =  - x\\dfrac {(e^{w^Tx})^2}{(1+e^{w^Tx})^{2}} + \\dfrac{1}{(1+e^{w^Tx})} \\dfrac{ \\partial e^{w^Tx}}{\\partial {w^Tx}} \\dfrac{ \\partial w^Tx}{\\partial w  }  $\n",
    "\n",
    "$\\dfrac{ \\partial \\hat{y} }{\\partial w} =  - x\\dfrac {(e^{w^Tx})^2}{(1+e^{w^Tx})^{2}} + \\dfrac{1}{(1+e^{w^Tx})} e^{w^Tx} x $ \n",
    "\n",
    "$\\dfrac{ \\partial \\hat{y} }{\\partial w} = x  \\dfrac{e^{w^Tx}}{(1+e^{w^Tx})}  [1 - \\dfrac {(e^{w^Tx})}{(1+e^{w^Tx})} ] $ \n",
    "\n",
    "$\\dfrac{ \\partial \\hat{y} }{\\partial w} = x  \\hat{y}  [1 - \\hat{y} ]  $\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\\dfrac{\\partial l(w,(x,y))}{\\partial w} = -  [ylog\\hat{y} + (1-y)log(1-\\hat{y}]$\n",
    "\n",
    "$\\dfrac{\\partial l(w,(x,y))}{\\partial w} = -  [y \\dfrac{\\partial log \\hat{y} }{\\partial \\hat{y}}  \\dfrac{ \\partial \\hat{y} }{\\partial w}  + (1-y) \\dfrac{ \\partial log(1-\\hat{y})}{\\partial(1- \\hat{y})}  \\dfrac{ \\partial (1-\\hat{y})}{\\partial(\\hat{y})}   \\dfrac{ \\partial \\hat{y} }{\\partial w}   ]$\n",
    "\n",
    "\n",
    "$\\dfrac{\\partial l(w,(x,y))}{\\partial w} = -  [y \\dfrac {\\partial log \\hat{y} }{\\partial \\hat{y}}    + (1-y) \\dfrac{ \\partial log(1-\\hat{y})}{\\partial(1- \\hat{y})}  \\dfrac{ \\partial (1-\\hat{y})}{\\partial(\\hat{y})} ] \\dfrac{ \\partial \\hat{y} }{\\partial w}$\n",
    "\n",
    "$\\dfrac{\\partial l(w,(x,y))}{\\partial w} = -  [ \\dfrac{y}{ \\hat{y}} - \\dfrac{ 1-y}{(1- \\hat{y})}  ] \\dfrac{ \\partial \\hat{y} }{\\partial w}$\n",
    "\n",
    "$\\dfrac{\\partial l(w,(x,y))}{\\partial w} = -  [ \\dfrac{y - y \\hat{y} - \\hat{y} + y \\hat{y} }{ \\hat{y} (1-\\hat{y}) } ] \\dfrac{ \\partial \\hat{y} }{\\partial w}$\n",
    "\n",
    "$\\dfrac{\\partial l(w,(x,y))}{\\partial w} = -  [ \\dfrac{y - \\hat{y}  }{ \\hat{y} (1-\\hat{y}) } ]  x \\hat{y}  (1 - \\hat{y})   $\n",
    "\n",
    "$\\dfrac{\\partial l(w,(x,y))}{\\partial w} =  ( \\hat{y} - y )x $\n",
    "\n",
    "\n",
    "For Gradient descent:\n",
    "\n",
    "Loop for epoch:\n",
    "    wt = wt - lr * grad; here grad = avg loss of all sample (10k here)\n",
    "    \n",
    "    \n",
    "    \n",
    "For Gradient descent:\n",
    "\n",
    "Loop for epoch:\n",
    "    loop for 1 to iterations: \n",
    "        wt = wt - lr * grad; here grad =  loss from 1 random sample\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Gradient Descent vs Stochastic Gradient Descent\n",
    "\n",
    "Discuss what do you observe about GD and SGD from your implementation? Which one is faster in terms of number of iterations, which one is faster in terms of the wall clock time?\n",
    "\n",
    "In case of GD, There is smooth convergence, which makes sense because it takes into account of all data. \n",
    "In case of SGD, there is up-down unsmooth convergences because for gradient only 1 random sample is taken into account. \n",
    "\n",
    "Intuitively, SGD should be faster than GD, because in GD, we take all samples gradient whereas in SGD only 1 sample.\n",
    "\n",
    "Attached is the plot of error vs epoch where,\n",
    "first plot is of SGD & second is of GD.\n",
    "\n",
    "<img src=\"SGD - Error vs Epoch-bow.png\">\n",
    "<img src=\"GD - Error vs Epoch-bow.png\">\n",
    "\n",
    "\n",
    "You can plot the learning curves, e.g., training error against epochs and wall-clock time.\n",
    "\n",
    "You can use matplotlib for plotting such figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Apply the model to your own text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructor of Extractor\n"
     ]
    }
   ],
   "source": [
    "from classifier import load_data,tokenize, compute_word_idf\n",
    "from classifier import custom_feature_extractor, classifier_agent\n",
    "import numpy as np\n",
    "\n",
    "# First load the classifier\n",
    "\n",
    "with open('data/vocab.txt') as file:\n",
    "    reading = file.readlines()\n",
    "    vocab_list = [item.strip() for item in reading]\n",
    "\n",
    "feat_map = custom_feature_extractor(vocab_list, tokenize)\n",
    "\n",
    "d = len(vocab_list)\n",
    "params = np.array([0.0 for i in range(d)])\n",
    "custom_classifier = classifier_agent(feat_map, params)\n",
    "custom_classifier.load_params_from_file('best_model.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [0] [0] [1]\n"
     ]
    }
   ],
   "source": [
    "# Try it out!\n",
    "\n",
    "my_sentence = \"This movie is amazing! Truly a masterpiece.\"\n",
    "\n",
    "my_sentence2 = \"The book is really, really good. The movie is just dreadful.\"\n",
    "my_sentence3 = \"This movie is worst, I don't like it.\"\n",
    "my_sentence4= \"Amazing, mind blowing.\"\n",
    "\n",
    "\n",
    "\n",
    "ypred = custom_classifier.predict(my_sentence,RAW_TEXT=True)\n",
    "ypred2 = custom_classifier.predict(my_sentence2,RAW_TEXT=True)\n",
    "ypred3 = custom_classifier.predict(my_sentence3,RAW_TEXT=True)\n",
    "ypred4 = custom_classifier.predict(my_sentence4,RAW_TEXT=True)\n",
    "\n",
    "\n",
    "\n",
    "print(ypred,ypred2, ypred3, ypred4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also try predicting for each word in the input so as to get a sense of how the classifier arrived at the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mw/m50sp3mn4151q1n3mldhw2fh0000gq/T/ipykernel_57922/1404405345.py:23: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  df.style.applymap(color_predictions)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_cb95e_row0_col0, #T_cb95e_row0_col1, #T_cb95e_row0_col2, #T_cb95e_row0_col3, #T_cb95e_row0_col4, #T_cb95e_row0_col5, #T_cb95e_row0_col6, #T_cb95e_row0_col7, #T_cb95e_row0_col8, #T_cb95e_row0_col9, #T_cb95e_row0_col10, #T_cb95e_row1_col0, #T_cb95e_row1_col1, #T_cb95e_row1_col2, #T_cb95e_row1_col3, #T_cb95e_row1_col4, #T_cb95e_row1_col5, #T_cb95e_row1_col6, #T_cb95e_row1_col7, #T_cb95e_row1_col8, #T_cb95e_row1_col9, #T_cb95e_row1_col10 {\n",
       "  color: black;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_cb95e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cb95e_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
       "      <th id=\"T_cb95e_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n",
       "      <th id=\"T_cb95e_level0_col2\" class=\"col_heading level0 col2\" >2</th>\n",
       "      <th id=\"T_cb95e_level0_col3\" class=\"col_heading level0 col3\" >3</th>\n",
       "      <th id=\"T_cb95e_level0_col4\" class=\"col_heading level0 col4\" >4</th>\n",
       "      <th id=\"T_cb95e_level0_col5\" class=\"col_heading level0 col5\" >5</th>\n",
       "      <th id=\"T_cb95e_level0_col6\" class=\"col_heading level0 col6\" >6</th>\n",
       "      <th id=\"T_cb95e_level0_col7\" class=\"col_heading level0 col7\" >7</th>\n",
       "      <th id=\"T_cb95e_level0_col8\" class=\"col_heading level0 col8\" >8</th>\n",
       "      <th id=\"T_cb95e_level0_col9\" class=\"col_heading level0 col9\" >9</th>\n",
       "      <th id=\"T_cb95e_level0_col10\" class=\"col_heading level0 col10\" >10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cb95e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_cb95e_row0_col0\" class=\"data row0 col0\" >the</td>\n",
       "      <td id=\"T_cb95e_row0_col1\" class=\"data row0 col1\" >book</td>\n",
       "      <td id=\"T_cb95e_row0_col2\" class=\"data row0 col2\" >is</td>\n",
       "      <td id=\"T_cb95e_row0_col3\" class=\"data row0 col3\" >really</td>\n",
       "      <td id=\"T_cb95e_row0_col4\" class=\"data row0 col4\" >really</td>\n",
       "      <td id=\"T_cb95e_row0_col5\" class=\"data row0 col5\" >good</td>\n",
       "      <td id=\"T_cb95e_row0_col6\" class=\"data row0 col6\" >the</td>\n",
       "      <td id=\"T_cb95e_row0_col7\" class=\"data row0 col7\" >movie</td>\n",
       "      <td id=\"T_cb95e_row0_col8\" class=\"data row0 col8\" >is</td>\n",
       "      <td id=\"T_cb95e_row0_col9\" class=\"data row0 col9\" >just</td>\n",
       "      <td id=\"T_cb95e_row0_col10\" class=\"data row0 col10\" >dreadful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cb95e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_cb95e_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_cb95e_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "      <td id=\"T_cb95e_row1_col2\" class=\"data row1 col2\" >1</td>\n",
       "      <td id=\"T_cb95e_row1_col3\" class=\"data row1 col3\" >1</td>\n",
       "      <td id=\"T_cb95e_row1_col4\" class=\"data row1 col4\" >1</td>\n",
       "      <td id=\"T_cb95e_row1_col5\" class=\"data row1 col5\" >1</td>\n",
       "      <td id=\"T_cb95e_row1_col6\" class=\"data row1 col6\" >1</td>\n",
       "      <td id=\"T_cb95e_row1_col7\" class=\"data row1 col7\" >1</td>\n",
       "      <td id=\"T_cb95e_row1_col8\" class=\"data row1 col8\" >1</td>\n",
       "      <td id=\"T_cb95e_row1_col9\" class=\"data row1 col9\" >0</td>\n",
       "      <td id=\"T_cb95e_row1_col10\" class=\"data row1 col10\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16d6afb10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# function for set text color of positive\n",
    "# values in Dataframes\n",
    "def color_predictions(val):\n",
    "    eps = 0.02\n",
    "    if isinstance(val,float):\n",
    "        if val > eps:\n",
    "            color = 'blue'\n",
    "        elif val < -eps:\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'black'\n",
    "    else:\n",
    "        color='black'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "my_sentence_list = tokenize(my_sentence2)\n",
    "ypred_per_word = custom_classifier.predict(my_sentence_list,RAW_TEXT=True,RETURN_SCORE=True)\n",
    "\n",
    "df = pd.DataFrame([my_sentence_list,ypred_per_word])\n",
    "\n",
    "df.style.applymap(color_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the questions: \n",
    "1. Are the above results making intuitive sense and why?\n",
    "\n",
    "Some of the findings i got is:\n",
    "When i increased the epochs to 1000 then the error was less. With 100 epoch, I got nearly 0.34 errors, whereas for 1000 i got 0.273. Attached graph is for 100 epochs only. With SGD, I was able to get higher accuracy(less error) with just 10 epoch.\n",
    "\n",
    "\n",
    "2. What are some limitation of a linear classifier with BoW features?\n",
    "\n",
    "Limitation would be, it might not be able to get semantics of larger context, lots of sparse relationship, might be unknown words\n",
    "\n",
    "3. what are some ideas you can come up with to overcome these limitations (i.e., what are your ideas of constructing informative features)?\n",
    "\n",
    "I tried to use nltk package for stop words, though the submitted code is without removing stop words, i have commented the code for the stop words.\n",
    "\n",
    "Lemmatization (where root of words can be found,like for running,runs, ran: run is lemma. Reducing vocab size by using lematizer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Document what you did for custom feature extractors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did you try? What were the accuracy you got. What worked better and what not, and why?\n",
    "\n",
    "I used tf-idf, the error were reduced, at last epoch in sgd, my error increased. I am yet to figure out why. Using tf-idf, gd accuracy was increased. In term doc frequency, there is penalty for those words which occur too much and some scaling factor for  words that occur rarely in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5:  Anything else you'd like to write about. Your instructor / TA will read them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may answer for instance:\n",
    "\n",
    "- What have you learned from the experience of working on this coding project?\n",
    "\n",
    "I learnt to implement the linear classifier, how gradient descent / SGD can be implemented. \n",
    "\n",
    "- Do you think it is easy / hard? If you find it to be hard, what is the main missing piece that you think the instructor / TA should cover in the lectures / discussion sections.\n",
    "\n",
    "It's medium for me. I was little frustrated out when my error was high. I took help from my friends (Vlad, Aditya).\n",
    "I am very grateful towards TA / instructor for providing the starter kit. It connected the dots. Thank you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
